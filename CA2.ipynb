{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec267e72",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2420b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2919e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Time Series Sentiment Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56bbe78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the data ProjectTweets.csv into hadoop in the named folder 'user1'\n",
    "df = spark.read.csv('/user1/ProjectTweets.csv', header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd37578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the structure of schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7b941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+---------------+--------------------+\n",
      "|index|   user_id|           timestamp|       username|          tweet_text|\n",
      "+-----+----------+--------------------+---------------+--------------------+\n",
      "|    0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|\n",
      "|    2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|\n",
      "|    3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|\n",
      "|    4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|\n",
      "|    5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|\n",
      "|    6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |\n",
      "|    7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|\n",
      "|    8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|    9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|\n",
      "|   10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|\n",
      "|   11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|\n",
      "|   12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|\n",
      "|   13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|\n",
      "|   14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|\n",
      "|   15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|\n",
      "|   16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|\n",
      "|   17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes |\n",
      "|   18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|\n",
      "|   19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+-----+----------+--------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Extract the first row which contains the original header information\n",
    "header_row = df.first()\n",
    "\n",
    "# Rename the columns as specified\n",
    "new_column_names = [\"index\", \"user_id\", \"timestamp\", \"query\", \"username\", \"tweet_text\"]\n",
    "for i, colname in enumerate(df.columns):\n",
    "    df = df.withColumnRenamed(colname, new_column_names[i])\n",
    "\n",
    "# Drop the first row from the DataFrame to avoid duplication\n",
    "df = df.filter(df.index != header_row[0])\n",
    "\n",
    "# Construct a new DataFrame with header row \n",
    "header_df = spark.createDataFrame([header_row], new_column_names)\n",
    "\n",
    "# Concatenate header DataFrame and original DataFrame\n",
    "df = header_df.union(df)\n",
    "\n",
    "# Drop the \"query\" column\n",
    "df = df.drop(\"query\")\n",
    "\n",
    "# Show the DataFrame to verify\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1753008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(index=0, user_id=0, timestamp=0, username=0, tweet_text=0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, count, col\n",
    "\n",
    "# Counting missing data\n",
    "missing_data_count = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).collect()\n",
    "missing_data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62cb3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows affected by the cleanup: 1599999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Initial data cleanup steps\n",
    "# Remove URLs\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"tweet_text\"), \"(http://[^\\\\s]+|https://[^\\\\s]+)\", \"\"))\n",
    "\n",
    "# Remove mentions\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"(@[\\\\w]+)\", \"\"))\n",
    "\n",
    "# Remove hashtags\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"(#[\\\\w]+)\", \"\"))\n",
    "\n",
    "# Remove other special characters (like &, *, %, etc.). Originally we have removed all ! and ? however I think this\n",
    "# will impact negativly the sentiment analysis so I decided to keep them in\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"[&*%$#@]+\", \"\"))\n",
    "\n",
    "# Removing multiple spaces left after removal\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"\\\\s+\", \" \"))\n",
    "\n",
    "# Trimming spaces at the beginning and the end\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"^\\\\s+|\\\\s+$\", \"\"))\n",
    "\n",
    "# Counting rows that had URLs, mentions, hashtags, and special characters removed\n",
    "affected_count = df.filter(col(\"tweet_text\") != col(\"cleaned_text\")).count()\n",
    "\n",
    "print(f\"Number of rows affected by the cleanup: {affected_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb27224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+---------------+--------------------+--------------------+\n",
      "|index|   user_id|           timestamp|       username|          tweet_text|        cleaned_text|\n",
      "+-----+----------+--------------------+---------------+--------------------+--------------------+\n",
      "|    0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|- Awww, that's a ...|\n",
      "|    1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|is upset that he ...|\n",
      "|    2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|I dived many time...|\n",
      "|    3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|my whole body fee...|\n",
      "|    4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|no, it's not beha...|\n",
      "|    5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|  not the whole crew|\n",
      "|    6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |          Need a hug|\n",
      "|    7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|hey long time no ...|\n",
      "|    8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|nope they didn't ...|\n",
      "|    9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|      que me muera ?|\n",
      "|   10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|spring break in p...|\n",
      "|   11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|I just re-pierced...|\n",
      "|   12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|I couldn't bear t...|\n",
      "|   13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|It it counts, idk...|\n",
      "|   14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|i would've been t...|\n",
      "|   15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|I wish I got to w...|\n",
      "|   16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|Hollis' death sce...|\n",
      "|   17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes | about to file taxes|\n",
      "|   18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|ahh ive always wa...|\n",
      "|   19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|Oh dear. Were you...|\n",
      "+-----+----------+--------------------+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c0fdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           timestamp|\n",
      "+--------------------+\n",
      "|Mon Apr 06 22:19:...|\n",
      "|Mon Apr 06 22:19:...|\n",
      "|Mon Apr 06 22:19:...|\n",
      "|Mon Apr 06 22:19:...|\n",
      "|Mon Apr 06 22:19:...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The datatype of the timestamp column is: StringType\n"
     ]
    }
   ],
   "source": [
    "# Check the datatype of the 'timestamp' column\n",
    "timestamp_type = df.schema[\"timestamp\"].dataType\n",
    "\n",
    "# Show the first few entries of the 'timestamp' column\n",
    "df.select(\"timestamp\").show(5)\n",
    "\n",
    "print(f\"The datatype of the timestamp column is: {timestamp_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d3dbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          timestamp|\n",
      "+-------------------+\n",
      "|2009-04-07 06:19:45|\n",
      "|2009-04-07 06:19:49|\n",
      "|2009-04-07 06:19:53|\n",
      "|2009-04-07 06:19:57|\n",
      "|2009-04-07 06:19:57|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:======================================>                   (2 + 1) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#The timestamp is in the format typically seen with Twitter data, which looks like \"Mon Apr 06 22:19:...\" \n",
    "# and is a StringType in the DataFrame. For time series analysis, it would be beneficial to convert this \n",
    "# StringType to a TimestampType in Spark.\n",
    "\n",
    "#Error while trying to convert the time: You may get a different result due to the upgrading of Spark 3.0: \n",
    "#Fail to recognize 'EEE MMM dd HH:mm:ss +SSSS yyyy' pattern in the DateTimeFormatter. \n",
    "#1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. \n",
    "#2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "# Correct the format to match the provided timestamps\n",
    "timestamp_format = \"EEE MMM dd HH:mm:ss zzz yyyy\"\n",
    "\n",
    "# Convert the string to timestamp type\n",
    "df = df.withColumn(\"timestamp\", \n",
    "                   from_unixtime(unix_timestamp(df[\"timestamp\"], timestamp_format)).cast(\"timestamp\"))\n",
    "\n",
    "# Show the converted timestamps\n",
    "df.select(\"timestamp\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd2d080d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        cleaned_text|\n",
      "+--------------------+\n",
      "|- Awww, that's a ...|\n",
      "|is upset that he ...|\n",
      "|I dived many time...|\n",
      "|my whole body fee...|\n",
      "|no, it's not beha...|\n",
      "|  not the whole crew|\n",
      "|          Need a hug|\n",
      "|hey long time no ...|\n",
      "|nope they didn't ...|\n",
      "|      que me muera ?|\n",
      "|spring break in p...|\n",
      "|I just re-pierced...|\n",
      "|I couldn't bear t...|\n",
      "|It it counts, idk...|\n",
      "|i would've been t...|\n",
      "|I wish I got to w...|\n",
      "|Hollis' death sce...|\n",
      "| about to file taxes|\n",
      "|ahh ive always wa...|\n",
      "|Oh dear. Were you...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"cleaned_text\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "234659b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        cleaned_text|\n",
      "+--------------------+\n",
      "|- awww, that's a ...|\n",
      "|is upset that he ...|\n",
      "|i dived many time...|\n",
      "|my whole body fee...|\n",
      "|no, it's not beha...|\n",
      "|  not the whole crew|\n",
      "|          need a hug|\n",
      "|hey long time no ...|\n",
      "|nope they didn't ...|\n",
      "|      que me muera ?|\n",
      "|spring break in p...|\n",
      "|i just re-pierced...|\n",
      "|i couldn't bear t...|\n",
      "|it it counts, idk...|\n",
      "|i would've been t...|\n",
      "|i wish i got to w...|\n",
      "|hollis' death sce...|\n",
      "| about to file taxes|\n",
      "|ahh ive always wa...|\n",
      "|oh dear. were you...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I decided to change all upper case letters to lower case to ensure consistency in the \n",
    "#text data and to reduce the dimensionality of the data\n",
    "\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "df = df.withColumn(\"cleaned_text\", lower(col(\"cleaned_text\")))\n",
    "\n",
    "df.select(\"cleaned_text\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1de23cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the original tweet_text and rename the cleaned_text\n",
    "\n",
    "df = df.drop(\"tweet_text\").withColumnRenamed(\"cleaned_text\", \"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68da9f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------+---------------+--------------------+\n",
      "|index|   user_id|          timestamp|       username|              tweets|\n",
      "+-----+----------+-------------------+---------------+--------------------+\n",
      "|    0|1467810369|2009-04-07 06:19:45|_TheSpecialOne_|- awww, that's a ...|\n",
      "|    1|1467810672|2009-04-07 06:19:49|  scotthamilton|is upset that he ...|\n",
      "|    2|1467810917|2009-04-07 06:19:53|       mattycus|i dived many time...|\n",
      "|    3|1467811184|2009-04-07 06:19:57|        ElleCTF|my whole body fee...|\n",
      "|    4|1467811193|2009-04-07 06:19:57|         Karoli|no, it's not beha...|\n",
      "+-----+----------+-------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f99928a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"user_id\", \"userid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bb138a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------+---------------+--------------------+\n",
      "|index|    userid|          timestamp|       username|              tweets|\n",
      "+-----+----------+-------------------+---------------+--------------------+\n",
      "|    0|1467810369|2009-04-07 06:19:45|_TheSpecialOne_|- awww, that's a ...|\n",
      "|    1|1467810672|2009-04-07 06:19:49|  scotthamilton|is upset that he ...|\n",
      "|    2|1467810917|2009-04-07 06:19:53|       mattycus|i dived many time...|\n",
      "|    3|1467811184|2009-04-07 06:19:57|        ElleCTF|my whole body fee...|\n",
      "|    4|1467811193|2009-04-07 06:19:57|         Karoli|no, it's not beha...|\n",
      "+-----+----------+-------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11e4b680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_rows = df.distinct().count()\n",
    "distinct_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef5e5c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no duplicate rows in the DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:===========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_rows = df.count()\n",
    "\n",
    "if total_rows > distinct_rows:\n",
    "    print(f\"There are {total_rows - distinct_rows} duplicate rows in the DataFrame.\")\n",
    "else:\n",
    "    print(\"There are no duplicate rows in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dfc28c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1588174"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Checking for duplicates based on 'username' and 'tweets'\n",
    "distinct_rows_based_on_columns = df.dropDuplicates(['username', 'tweets'])\n",
    "distinct_rows_based_on_columns.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bef37d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------+---------------+--------------------+\n",
      "|  index|    userid|          timestamp|       username|              tweets|\n",
      "+-------+----------+-------------------+---------------+--------------------+\n",
      "| 867691|1677811976|2009-05-02 12:07:10|   lopezwilfred|thanks for following|\n",
      "| 984024|1834348378|2009-05-18 10:35:33|    pacmanangel|'time is an illus...|\n",
      "|1179643|1981772490|2009-05-31 17:51:42|    what_bugs_u|why limit your st...|\n",
      "| 109491|1824573027|2009-05-17 09:58:37|  TheAmazingCat|cant afford to se...|\n",
      "| 153137|1932894690|2009-05-27 06:28:32|    _magic8ball|   don't count on it|\n",
      "| 537360|2198731504|2009-06-17 00:29:31|         Keys10|wish i were there...|\n",
      "| 760094|2296489742|2009-06-23 17:14:52|    Beth_Wilson|great work! plz k...|\n",
      "|1009141|1880869710|2009-05-22 10:47:41|  mustntgrumble|              thanks|\n",
      "|1418311|2057764859|2009-06-06 21:37:07| tweeteradder15|get 100 followers...|\n",
      "|1473158|2065508379|2009-06-07 16:44:27|   19c816tf9227|dannygokey's seri...|\n",
      "|1585412|2190679058|2009-06-16 11:25:14|        fredwin|naa mate i get ex...|\n",
      "| 329930|2011691828|2009-06-03 03:25:06|      milamcfly|you have not been...|\n",
      "| 364679|2048272376|2009-06-05 22:56:13|       lost_dog|i am lost. please...|\n",
      "| 980117|1833993661|2009-05-18 09:13:11|DannysFoxySmile|hi, i'm foxysmile...|\n",
      "|1262998|1998942722|2009-06-02 03:22:49|    what_bugs_u|why limit your st...|\n",
      "|  46471|1677422623|2009-05-02 10:00:07|       tweetpet|           clean me!|\n",
      "| 753859|2287027757|2009-06-23 01:15:09|       lost_dog|i am lost. please...|\n",
      "| 777643|2322472273|2009-06-25 06:40:42|   sierrabardot|you saved my life...|\n",
      "|1070347|1966113683|2009-05-30 01:08:59|   livin4adream|emarketcoach: fol...|\n",
      "|1294571|2003535748|2009-06-02 14:42:38|    what_bugs_u|why limit your st...|\n",
      "+-------+----------+-------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Identify the duplicate rows by subtracting distinct rows from the original DataFrame\n",
    "duplicate_rows = df.subtract(distinct_rows_based_on_columns)\n",
    "\n",
    "# Display the duplicate rows\n",
    "duplicate_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c2eee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11826"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_rows.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d138f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'index' column\n",
    "df = df.drop('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7426f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1588174"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_rows_based_on_columns = df.dropDuplicates(['username', 'tweets'])\n",
    "distinct_rows_based_on_columns.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "119a9832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:====================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------------+--------------------+\n",
      "|    userid|          timestamp|       username|              tweets|\n",
      "+----------+-------------------+---------------+--------------------+\n",
      "|1573784974|2009-04-21 10:50:48|    PatrickAnna|patrick is trying...|\n",
      "|1833723051|2009-05-18 08:13:56|Bookwhisperer37|- this is my sad ...|\n",
      "|1967499708|2009-05-30 03:41:39|    onlyrob1402|y!oi thay doi khi...|\n",
      "|1978763438|2009-05-31 08:23:11|    Camila_love|why i can't chang...|\n",
      "|1982354851|2009-05-31 19:02:35|   jillinthe408|where did this so...|\n",
      "|1994041291|2009-06-01 19:19:16|  JasmineBarton|please please ple...|\n",
      "|2066027792|2009-06-07 17:45:07|  tweeteradder8|get 100 followers...|\n",
      "|2068415636|2009-06-07 22:02:32|      RiceMover|allow me to answe...|\n",
      "|2183616999|2009-06-15 22:45:09|       lost_dog|i am lost. please...|\n",
      "|2195286993|2009-06-16 18:55:52|   elisabeth_L2|jogging, isn't re...|\n",
      "|1836264146|2009-05-18 15:41:02|   Youtube__Com|best of youtube: ...|\n",
      "|1963733051|2009-05-29 21:09:58|       cycoivan|not making good t...|\n",
      "|1970251338|2009-05-30 11:04:58|    SomersetBob|              thanks|\n",
      "|1970494672|2009-05-30 12:05:35|      keren4562|plz say quot;happ...|\n",
      "|1989146196|2009-06-01 08:30:03|     iamatechie|i am busy with wo...|\n",
      "|2050040223|2009-06-06 01:57:36| KevinEdwardsJr|if you like to la...|\n",
      "|2051994237|2009-06-06 07:21:16|   CathrynMarie|                    |\n",
      "|2057710596|2009-06-06 21:31:00|       analara_|                -day|\n",
      "|2182528209|2009-06-15 21:17:25|  tweeteradder2|get 100 followers...|\n",
      "|2187966544|2009-06-16 05:05:00|    _magic8ball|   my sources say no|\n",
      "+----------+-------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Identify the duplicate rows by subtracting distinct rows from the original DataFrame\n",
    "duplicate_rows = df.subtract(distinct_rows_based_on_columns)\n",
    "\n",
    "# Display the duplicate rows\n",
    "duplicate_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e56d6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10141"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_rows.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee998c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
