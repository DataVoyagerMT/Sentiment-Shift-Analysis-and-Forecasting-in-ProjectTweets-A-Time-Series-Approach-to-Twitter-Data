{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec267e72",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2420b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2919e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Time Series Sentiment Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56bbe78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the data ProjectTweets.csv into hadoop in the named folder 'user1'\n",
    "df = spark.read.csv('/user1/ProjectTweets.csv', header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd37578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the structure of schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7b941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+---------------+--------------------+\n",
      "|index|   user_id|           timestamp|       username|          tweet_text|\n",
      "+-----+----------+--------------------+---------------+--------------------+\n",
      "|    0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|\n",
      "|    2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|\n",
      "|    3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|\n",
      "|    4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|\n",
      "|    5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|\n",
      "|    6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |\n",
      "|    7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|\n",
      "|    8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|    9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|\n",
      "|   10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|\n",
      "|   11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|\n",
      "|   12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|\n",
      "|   13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|\n",
      "|   14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|\n",
      "|   15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|\n",
      "|   16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|\n",
      "|   17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes |\n",
      "|   18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|\n",
      "|   19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+-----+----------+--------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Extract the first row which contains the original header information\n",
    "header_row = df.first()\n",
    "\n",
    "# Rename the columns as specified\n",
    "new_column_names = [\"index\", \"user_id\", \"timestamp\", \"query\", \"username\", \"tweet_text\"]\n",
    "for i, colname in enumerate(df.columns):\n",
    "    df = df.withColumnRenamed(colname, new_column_names[i])\n",
    "\n",
    "# Drop the first row from the DataFrame to avoid duplication\n",
    "df = df.filter(df.index != header_row[0])\n",
    "\n",
    "# Construct a new DataFrame with header row \n",
    "header_df = spark.createDataFrame([header_row], new_column_names)\n",
    "\n",
    "# Concatenate header DataFrame and original DataFrame\n",
    "df = header_df.union(df)\n",
    "\n",
    "# Drop the \"query\" column\n",
    "df = df.drop(\"query\")\n",
    "\n",
    "# Show the DataFrame to verify\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04179bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(index=0, user_id=0, timestamp=0, username=0, tweet_text=0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, count, col\n",
    "\n",
    "# Counting missing data\n",
    "missing_data_count = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).collect()\n",
    "missing_data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62cb3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows affected by the cleanup: 1600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Initial data cleanup steps\n",
    "# Remove URLs\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"tweet_text\"), \"(http://[^\\\\s]+|https://[^\\\\s]+)\", \"\"))\n",
    "\n",
    "# Remove mentions\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"(@[\\\\w]+)\", \"\"))\n",
    "\n",
    "# Remove hashtags\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"(#[\\\\w]+)\", \"\"))\n",
    "\n",
    "# Remove other special characters (like &, *, %, etc.)\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"[&*%$#@!?]+\", \"\"))\n",
    "\n",
    "# Removing multiple spaces left after removal\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"\\\\s+\", \" \"))\n",
    "\n",
    "# Trimming spaces at the beginning and the end\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"^\\\\s+|\\\\s+$\", \"\"))\n",
    "\n",
    "# Counting rows that had URLs, mentions, hashtags, and special characters removed\n",
    "affected_count = df.filter(col(\"tweet_text\") != col(\"cleaned_text\")).count()\n",
    "\n",
    "print(f\"Number of rows affected by the cleanup: {affected_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd5da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+---------------+--------------------+--------------------+\n",
      "|index|   user_id|           timestamp|       username|          tweet_text|        cleaned_text|\n",
      "+-----+----------+--------------------+---------------+--------------------+--------------------+\n",
      "|    0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|- Awww, that's a ...|\n",
      "|    1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|is upset that he ...|\n",
      "|    2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|I dived many time...|\n",
      "|    3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|my whole body fee...|\n",
      "|    4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|no, it's not beha...|\n",
      "|    5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|  not the whole crew|\n",
      "|    6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |          Need a hug|\n",
      "|    7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|hey long time no ...|\n",
      "|    8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|nope they didn't ...|\n",
      "|    9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|        que me muera|\n",
      "|   10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|spring break in p...|\n",
      "|   11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|I just re-pierced...|\n",
      "|   12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|I couldn't bear t...|\n",
      "|   13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|It it counts, idk...|\n",
      "|   14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|i would've been t...|\n",
      "|   15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|I wish I got to w...|\n",
      "|   16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|Hollis' death sce...|\n",
      "|   17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes | about to file taxes|\n",
      "|   18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|ahh ive always wa...|\n",
      "|   19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|Oh dear. Were you...|\n",
      "+-----+----------+--------------------+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2bac88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
