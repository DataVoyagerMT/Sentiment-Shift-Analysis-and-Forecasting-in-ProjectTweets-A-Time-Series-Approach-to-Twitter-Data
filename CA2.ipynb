{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec267e72",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2420b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2919e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Time Series Sentiment Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56bbe78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the data ProjectTweets.csv into hadoop in the named folder 'user1'\n",
    "df = spark.read.csv('/user1/ProjectTweets.csv', header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd37578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the structure of schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7b941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+---------------+--------------------+\n",
      "|index|   user_id|           timestamp|       username|          tweet_text|\n",
      "+-----+----------+--------------------+---------------+--------------------+\n",
      "|    0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|\n",
      "|    2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|\n",
      "|    3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|\n",
      "|    4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|\n",
      "|    5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|\n",
      "|    6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |\n",
      "|    7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|\n",
      "|    8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|    9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|\n",
      "|   10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|\n",
      "|   11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|\n",
      "|   12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|\n",
      "|   13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|\n",
      "|   14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|\n",
      "|   15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|\n",
      "|   16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|\n",
      "|   17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes |\n",
      "|   18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|\n",
      "|   19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+-----+----------+--------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Extract the first row which contains the original header information\n",
    "header_row = df.first()\n",
    "\n",
    "# Rename the columns as specified\n",
    "new_column_names = [\"index\", \"user_id\", \"timestamp\", \"query\", \"username\", \"tweet_text\"]\n",
    "for i, colname in enumerate(df.columns):\n",
    "    df = df.withColumnRenamed(colname, new_column_names[i])\n",
    "\n",
    "# Drop the first row from the DataFrame to avoid duplication\n",
    "df = df.filter(df.index != header_row[0])\n",
    "\n",
    "# Construct a new DataFrame with header row \n",
    "header_df = spark.createDataFrame([header_row], new_column_names)\n",
    "\n",
    "# Concatenate header DataFrame and original DataFrame\n",
    "df = header_df.union(df)\n",
    "\n",
    "# Drop the \"query\" column\n",
    "df = df.drop(\"query\")\n",
    "\n",
    "# Show the DataFrame to verify\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1753008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(index=0, user_id=0, timestamp=0, username=0, tweet_text=0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, count, col\n",
    "\n",
    "# Counting missing data\n",
    "missing_data_count = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).collect()\n",
    "missing_data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62cb3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows affected by the cleanup: 1599999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Initial data cleanup steps\n",
    "# Remove URLs\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"tweet_text\"), \"(http://[^\\\\s]+|https://[^\\\\s]+)\", \"\"))\n",
    "\n",
    "# Remove mentions\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"(@[\\\\w]+)\", \"\"))\n",
    "\n",
    "# Remove hashtags\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"(#[\\\\w]+)\", \"\"))\n",
    "\n",
    "# Remove other special characters (like &, *, %, etc.). Originally we have removed all ! and ? however I think this\n",
    "# will impact negativly the sentiment analysis so I decided to keep them in\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"[&*%$#@]+\", \"\"))\n",
    "\n",
    "# Removing multiple spaces left after removal\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"\\\\s+\", \" \"))\n",
    "\n",
    "# Trimming spaces at the beginning and the end\n",
    "df = df.withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), \"^\\\\s+|\\\\s+$\", \"\"))\n",
    "\n",
    "# Counting rows that had URLs, mentions, hashtags, and special characters removed\n",
    "affected_count = df.filter(col(\"tweet_text\") != col(\"cleaned_text\")).count()\n",
    "\n",
    "print(f\"Number of rows affected by the cleanup: {affected_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb27224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+---------------+--------------------+--------------------+\n",
      "|index|   user_id|           timestamp|       username|          tweet_text|        cleaned_text|\n",
      "+-----+----------+--------------------+---------------+--------------------+--------------------+\n",
      "|    0|1467810369|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|- Awww, that's a ...|\n",
      "|    1|1467810672|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|is upset that he ...|\n",
      "|    2|1467810917|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|I dived many time...|\n",
      "|    3|1467811184|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|my whole body fee...|\n",
      "|    4|1467811193|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|no, it's not beha...|\n",
      "|    5|1467811372|Mon Apr 06 22:20:...|       joy_wolf|@Kwesidei not the...|  not the whole crew|\n",
      "|    6|1467811592|Mon Apr 06 22:20:...|        mybirch|         Need a hug |          Need a hug|\n",
      "|    7|1467811594|Mon Apr 06 22:20:...|           coZZ|@LOLTrish hey  lo...|hey long time no ...|\n",
      "|    8|1467811795|Mon Apr 06 22:20:...|2Hood4Hollywood|@Tatiana_K nope t...|nope they didn't ...|\n",
      "|    9|1467812025|Mon Apr 06 22:20:...|        mimismo|@twittera que me ...|      que me muera ?|\n",
      "|   10|1467812416|Mon Apr 06 22:20:...| erinx3leannexo|spring break in p...|spring break in p...|\n",
      "|   11|1467812579|Mon Apr 06 22:20:...|   pardonlauren|I just re-pierced...|I just re-pierced...|\n",
      "|   12|1467812723|Mon Apr 06 22:20:...|           TLeC|@caregiving I cou...|I couldn't bear t...|\n",
      "|   13|1467812771|Mon Apr 06 22:20:...|robrobbierobert|@octolinz16 It it...|It it counts, idk...|\n",
      "|   14|1467812784|Mon Apr 06 22:20:...|    bayofwolves|@smarrison i woul...|i would've been t...|\n",
      "|   15|1467812799|Mon Apr 06 22:20:...|     HairByJess|@iamjazzyfizzle I...|I wish I got to w...|\n",
      "|   16|1467812964|Mon Apr 06 22:20:...| lovesongwriter|Hollis' death sce...|Hollis' death sce...|\n",
      "|   17|1467813137|Mon Apr 06 22:20:...|       armotley|about to file taxes | about to file taxes|\n",
      "|   18|1467813579|Mon Apr 06 22:20:...|     starkissed|@LettyA ahh ive a...|ahh ive always wa...|\n",
      "|   19|1467813782|Mon Apr 06 22:20:...|      gi_gi_bee|@FakerPattyPattz ...|Oh dear. Were you...|\n",
      "+-----+----------+--------------------+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c0fdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           timestamp|\n",
      "+--------------------+\n",
      "|Mon Apr 06 22:19:...|\n",
      "|Mon Apr 06 22:19:...|\n",
      "|Mon Apr 06 22:19:...|\n",
      "|Mon Apr 06 22:19:...|\n",
      "|Mon Apr 06 22:19:...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The datatype of the timestamp column is: StringType\n"
     ]
    }
   ],
   "source": [
    "# Check the datatype of the 'timestamp' column\n",
    "timestamp_type = df.schema[\"timestamp\"].dataType\n",
    "\n",
    "# Show the first few entries of the 'timestamp' column\n",
    "df.select(\"timestamp\").show(5)\n",
    "\n",
    "print(f\"The datatype of the timestamp column is: {timestamp_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d3dbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          timestamp|\n",
      "+-------------------+\n",
      "|2009-04-07 06:19:45|\n",
      "|2009-04-07 06:19:49|\n",
      "|2009-04-07 06:19:53|\n",
      "|2009-04-07 06:19:57|\n",
      "|2009-04-07 06:19:57|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The timestamp is in the format typically seen with Twitter data, which looks like \"Mon Apr 06 22:19:...\" \n",
    "# and is a StringType in the DataFrame. For time series analysis, it would be beneficial to convert this \n",
    "# StringType to a TimestampType in Spark.\n",
    "\n",
    "#Error while trying to convert the time: You may get a different result due to the upgrading of Spark 3.0: \n",
    "#Fail to recognize 'EEE MMM dd HH:mm:ss +SSSS yyyy' pattern in the DateTimeFormatter. \n",
    "#1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. \n",
    "#2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "# Correct the format to match the provided timestamps\n",
    "timestamp_format = \"EEE MMM dd HH:mm:ss zzz yyyy\"\n",
    "\n",
    "# Convert the string to timestamp type\n",
    "df = df.withColumn(\"timestamp\", \n",
    "                   from_unixtime(unix_timestamp(df[\"timestamp\"], timestamp_format)).cast(\"timestamp\"))\n",
    "\n",
    "# Show the converted timestamps\n",
    "df.select(\"timestamp\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd2d080d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        cleaned_text|\n",
      "+--------------------+\n",
      "|- Awww, that's a ...|\n",
      "|is upset that he ...|\n",
      "|I dived many time...|\n",
      "|my whole body fee...|\n",
      "|no, it's not beha...|\n",
      "|  not the whole crew|\n",
      "|          Need a hug|\n",
      "|hey long time no ...|\n",
      "|nope they didn't ...|\n",
      "|      que me muera ?|\n",
      "|spring break in p...|\n",
      "|I just re-pierced...|\n",
      "|I couldn't bear t...|\n",
      "|It it counts, idk...|\n",
      "|i would've been t...|\n",
      "|I wish I got to w...|\n",
      "|Hollis' death sce...|\n",
      "| about to file taxes|\n",
      "|ahh ive always wa...|\n",
      "|Oh dear. Were you...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"cleaned_text\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85e0b47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        cleaned_text|\n",
      "+--------------------+\n",
      "|- awww, that's a ...|\n",
      "|is upset that he ...|\n",
      "|i dived many time...|\n",
      "|my whole body fee...|\n",
      "|no, it's not beha...|\n",
      "|  not the whole crew|\n",
      "|          need a hug|\n",
      "|hey long time no ...|\n",
      "|nope they didn't ...|\n",
      "|      que me muera ?|\n",
      "|spring break in p...|\n",
      "|i just re-pierced...|\n",
      "|i couldn't bear t...|\n",
      "|it it counts, idk...|\n",
      "|i would've been t...|\n",
      "|i wish i got to w...|\n",
      "|hollis' death sce...|\n",
      "| about to file taxes|\n",
      "|ahh ive always wa...|\n",
      "|oh dear. were you...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I decided to change all upper case letters to lower case to ensure consistency in the \n",
    "#text data and to reduce the dimensionality of the data\n",
    "\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "df = df.withColumn(\"cleaned_text\", lower(col(\"cleaned_text\")))\n",
    "\n",
    "df.select(\"cleaned_text\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9276805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the original tweet_text and rename the cleaned_text\n",
    "\n",
    "df = df.drop(\"tweet_text\").withColumnRenamed(\"cleaned_text\", \"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91c5041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------+---------------+--------------------+\n",
      "|index|   user_id|          timestamp|       username|              tweets|\n",
      "+-----+----------+-------------------+---------------+--------------------+\n",
      "|    0|1467810369|2009-04-07 06:19:45|_TheSpecialOne_|- awww, that's a ...|\n",
      "|    1|1467810672|2009-04-07 06:19:49|  scotthamilton|is upset that he ...|\n",
      "|    2|1467810917|2009-04-07 06:19:53|       mattycus|i dived many time...|\n",
      "|    3|1467811184|2009-04-07 06:19:57|        ElleCTF|my whole body fee...|\n",
      "|    4|1467811193|2009-04-07 06:19:57|         Karoli|no, it's not beha...|\n",
      "+-----+----------+-------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcdf66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"user_id\", \"userid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d790b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------+---------------+--------------------+\n",
      "|index|    userid|          timestamp|       username|              tweets|\n",
      "+-----+----------+-------------------+---------------+--------------------+\n",
      "|    0|1467810369|2009-04-07 06:19:45|_TheSpecialOne_|- awww, that's a ...|\n",
      "|    1|1467810672|2009-04-07 06:19:49|  scotthamilton|is upset that he ...|\n",
      "|    2|1467810917|2009-04-07 06:19:53|       mattycus|i dived many time...|\n",
      "|    3|1467811184|2009-04-07 06:19:57|        ElleCTF|my whole body fee...|\n",
      "|    4|1467811193|2009-04-07 06:19:57|         Karoli|no, it's not beha...|\n",
      "+-----+----------+-------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9fe79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
