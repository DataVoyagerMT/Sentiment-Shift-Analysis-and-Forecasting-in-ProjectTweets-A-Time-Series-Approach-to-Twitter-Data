{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The error message ParserError, indicates that there is a mismatch in the number of columns expected and the number of columns\n",
    "#found in my CSV file. In this case, it expected 4 fields in line 19673 but found 5.\n",
    "#This type of error probably occurs when there are commas within a field that aren't enclosed in double quotes, causing the parser \n",
    "#to incorrectly interpret them as field separators. The other cleaned dataset will be used going forward\n",
    "#df_clean = pd.read_table('cleaned_ProjectTweets.csv', sep=',', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467812025</td>\n",
       "      <td>2009-04-07T06:20:09.000+01:00</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>que me muera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1468032862</td>\n",
       "      <td>2009-04-07T07:21:35.000+01:00</td>\n",
       "      <td>lamarmcarter</td>\n",
       "      <td>happy for coach stringer hof co now if i can o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1468043104</td>\n",
       "      <td>2009-04-07T07:24:53.000+01:00</td>\n",
       "      <td>Geoffasaurus</td>\n",
       "      <td>keeeerrrrriiiiii i really have nothing better ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1468078493</td>\n",
       "      <td>2009-04-07T07:35:44.000+01:00</td>\n",
       "      <td>kennyduduxd</td>\n",
       "      <td>cant sleep dam naps lol and its hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1468242583</td>\n",
       "      <td>2009-04-07T08:30:07.000+01:00</td>\n",
       "      <td>Tsukihysteria</td>\n",
       "      <td>thank you for being a great character on house...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userid                      timestamp       username  \\\n",
       "0  1467812025  2009-04-07T06:20:09.000+01:00        mimismo   \n",
       "1  1468032862  2009-04-07T07:21:35.000+01:00   lamarmcarter   \n",
       "2  1468043104  2009-04-07T07:24:53.000+01:00   Geoffasaurus   \n",
       "3  1468078493  2009-04-07T07:35:44.000+01:00    kennyduduxd   \n",
       "4  1468242583  2009-04-07T08:30:07.000+01:00  Tsukihysteria   \n",
       "\n",
       "                                               tweet  \n",
       "0                                       que me muera  \n",
       "1  happy for coach stringer hof co now if i can o...  \n",
       "2  keeeerrrrriiiiii i really have nothing better ...  \n",
       "3                cant sleep dam naps lol and its hot  \n",
       "4  thank you for being a great character on house...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaner = pd.read_csv('cleanedt_ProjectTweets.csv', dtype={0: str}) #the header was changed to string type\n",
    "\n",
    "df_cleaner.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467812025</td>\n",
       "      <td>2009-04-07T06:20:09.000+01:00</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>que me muera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1468032862</td>\n",
       "      <td>2009-04-07T07:21:35.000+01:00</td>\n",
       "      <td>lamarmcarter</td>\n",
       "      <td>happy for coach stringer hof co now if i can o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1468043104</td>\n",
       "      <td>2009-04-07T07:24:53.000+01:00</td>\n",
       "      <td>Geoffasaurus</td>\n",
       "      <td>keeeerrrrriiiiii i really have nothing better ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1468078493</td>\n",
       "      <td>2009-04-07T07:35:44.000+01:00</td>\n",
       "      <td>kennyduduxd</td>\n",
       "      <td>cant sleep dam naps lol and its hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1468242583</td>\n",
       "      <td>2009-04-07T08:30:07.000+01:00</td>\n",
       "      <td>Tsukihysteria</td>\n",
       "      <td>thank you for being a great character on house...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userid                      timestamp       username  \\\n",
       "0  1467812025  2009-04-07T06:20:09.000+01:00        mimismo   \n",
       "1  1468032862  2009-04-07T07:21:35.000+01:00   lamarmcarter   \n",
       "2  1468043104  2009-04-07T07:24:53.000+01:00   Geoffasaurus   \n",
       "3  1468078493  2009-04-07T07:35:44.000+01:00    kennyduduxd   \n",
       "4  1468242583  2009-04-07T08:30:07.000+01:00  Tsukihysteria   \n",
       "\n",
       "                                               tweet  \n",
       "0                                       que me muera  \n",
       "1  happy for coach stringer hof co now if i can o...  \n",
       "2  keeeerrrrriiiiii i really have nothing better ...  \n",
       "3                cant sleep dam naps lol and its hot  \n",
       "4  thank you for being a great character on house...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_cleaner\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\swast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\swast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\swast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\swast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Common Words in Frequency Distribution:\n",
      "im: 177631\n",
      "good: 88220\n",
      "day: 83371\n",
      "get: 79737\n",
      "like: 77263\n",
      "go: 72704\n",
      "got: 69342\n",
      "dont: 67003\n",
      "today: 64367\n",
      "going: 63907\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter  \n",
    "from nltk.probability import FreqDist \n",
    "from nltk.text import Text\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')  \n",
    "nltk.download('vader_lexicon') \n",
    "nltk.download('punkt') \n",
    "\n",
    "# NLTK operations\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Getting all tweets separated by a space\n",
    "all_tweets = df['tweet'].str.cat(sep=' ')\n",
    "\n",
    "# Tokenize\n",
    "tkwords = word_tokenize(all_tweets)\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "filtered_words = [word for word in tkwords if word.isalpha() and word.lower() not in stopwords]\n",
    "\n",
    "# Get the frequency distribution\n",
    "fd = FreqDist(filtered_words)  \n",
    "\n",
    "# Print the 10 most common words\n",
    "print(\"Top 10 Most Common Words in Frequency Distribution:\")\n",
    "for word, frequency in fd.most_common(10):\n",
    "    print(f\"{word}: {frequency}\")\n",
    "#The above code concatenates all the tweets into a single string, tokenizes the string into words, \n",
    "#filters out stopwords and non-alphabetic tokens, and then calculates and prints the 10 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concordances for word 'good':\n",
      "Displaying 5 of 88220 matches:\n",
      " time moment coach forced retirement good years ago due injury coach us havent\n",
      "ndstill today sorry bout car btw kat good vacation personally st year new jobz\n",
      "d ownmust life shite studying finals good ol stock valuation options amp futur\n",
      " grocery shopping lead exciting life good thing riding bike keeps blowing head\n",
      "igting twit curse twitter curse oooh good im fine thanks baking cricket lookin\n"
     ]
    }
   ],
   "source": [
    "# Create an NLTK Text object from filtered words\n",
    "text_obj = Text(filtered_words)\n",
    "\n",
    "# Specific word to find concordances with\n",
    "specific_word = 'good'  \n",
    "print(f\"\\nConcordances for word '{specific_word}':\")\n",
    "text_obj.concordance(specific_word, lines=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concordances for word 'happy':\n",
      "Displaying 5 of 26216 matches:\n",
      "que muera happy coach stringer hof co finish term pa\n",
      "ooooo twitter news weathers fab work happy raining going lena day sucks trying \n",
      "o hate public transportation raining happy monday alright got cold think feelin\n",
      " khong xoaquot sign week argh boo oh happy days australiain recsion thought alr\n",
      "ood snake house futsal week feel fat happy mothers day beautiful woman especial\n"
     ]
    }
   ],
   "source": [
    "specific_word = 'happy'  \n",
    "print(f\"\\nConcordances for word '{specific_word}':\")\n",
    "text_obj.concordance(specific_word, lines=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concordances for word 'great':\n",
      "Displaying 5 of 32712 matches:\n",
      "ot cant sleep dam naps lol hot thank great character house im sad see go wan na\n",
      "time realise talk cruise summer tour great making us fans europe sad xx holy sh\n",
      "ly going outside dont want dont feel great hate go away keep missing first part\n",
      " holy crap im making progress sounds great wish could go take photos web sites \n",
      " show drop text saying hes part crew great birds moms place sounds like inside \n"
     ]
    }
   ],
   "source": [
    "specific_word = 'great'  \n",
    "print(f\"\\nConcordances for word '{specific_word}':\")\n",
    "text_obj.concordance(specific_word, lines=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concordances for word 'wonderful':\n",
      "Displaying 5 of 3808 matches:\n",
      "tart goin back forreall lol sounds wonderful envy havent camped years think gla\n",
      "ill haent prebooked ooh sun always wonderful cloudy storms bowling green kentuc\n",
      " stupid lady cut bangs short grrrr wonderful asked sad pics etc see something s\n",
      "npacking weather dancing edinburgh wonderful morning happy mothers daymy mommy \n",
      "er shame guys would loved see sign wonderful wekend stockholm unfortenately cam\n"
     ]
    }
   ],
   "source": [
    "specific_word = 'wonderful'  \n",
    "print(f\"\\nConcordances for word '{specific_word}':\")\n",
    "text_obj.concordance(specific_word, lines=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concordances for word 'stocks':\n",
      "Displaying 5 of 44 matches:\n",
      " years think passing global fishing stocks fully exploited amp fish farms pose\n",
      "s saturday guess every body selling stocks results jessi likey fob sorry files\n",
      "tting sick cant believe forever ran stocks headband ugh dying one cant seem ge\n",
      "ing database ltltlt tried learn day stocks trading friday slownot best day tak\n",
      "iran nope saving burnout looks like stocks ran fifa would good bought weeks ba\n"
     ]
    }
   ],
   "source": [
    "# This is just a test for my own knowledge \n",
    "specific_word = 'stocks'  \n",
    "print(f\"\\nConcordances for word '{specific_word}':\")\n",
    "text_obj.concordance(specific_word, lines=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concordances for word 'bad':\n",
      "Displaying 5 of 26657 matches:\n",
      "abrinas im bored work haha already im bad student facebook tard cant get hold c\n",
      " hours think im gettin cold day going bad worsei broke shoe morning windy freez\n",
      "eadache wore cuddle shirt got cuddles bad im sick cant enjoy killers like wan n\n",
      "ize fixe bro u stop followin traffics bad guess prayers impossible answer chang\n",
      "feeling well cancel tuition noor felt bad sighs need buck things literally yes \n"
     ]
    }
   ],
   "source": [
    "specific_word = 'bad'  \n",
    "print(f\"\\nConcordances for word '{specific_word}':\")\n",
    "text_obj.concordance(specific_word, lines=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concordances for word 'upset':\n",
      "Displaying 5 of 2645 matches:\n",
      " dont know play cricket u got yuvraj upset cant find pair sexy cutout sandal bo\n",
      "e creepy chan win antm favorite sooo upset missed little rascals tv today love \n",
      "think one person work would ask went upset near tears spoke rob moring pouring \n",
      "y life rocksall need chris bizzle im upset didnt get see might kill green turtl\n",
      "rytools runtergeladen der server ist upset sun rain instead howards auditioning\n"
     ]
    }
   ],
   "source": [
    "specific_word = 'upset'  \n",
    "print(f\"\\nConcordances for word '{specific_word}':\")\n",
    "text_obj.concordance(specific_word, lines=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concordances for word 'modest':\n",
      "Displaying 5 of 46 matches:\n",
      "ht tired jb today show miss hearing modest mouse wilma cant buy tickets get co\n",
      "y end day oh neee drunkenly singing modest mouse top lungs wonder neighbors co\n",
      "ater didnt get told us well getting modest gift year noooooooo wish saw advert\n",
      "oftie today im lameee informed need modest wear full piece bathing suit im ok \n",
      "an fish shop gawping bewbs violated modest mouse touring august uk dates yet h\n"
     ]
    }
   ],
   "source": [
    "specific_word = 'modest'  \n",
    "print(f\"\\nConcordances for word '{specific_word}':\")\n",
    "text_obj.concordance(specific_word, lines=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concordances for word 'horrible':\n",
      "Displaying 5 of 2847 matches:\n",
      "wednesday around end may im gon na horrible time realise talk cruise summer to\n",
      " signing nowww school tmr sad must horrible families charles de gaulle airport\n",
      "ope didnt work imma sleep tomorrow horrible day ovr nite nite twitterlings okm\n",
      "ad cold today geuss overbasketball horrible pain shin splint right leg gon na \n",
      "y time forgot hug say goodbye feel horrible wont see whole week ive officially\n"
     ]
    }
   ],
   "source": [
    "specific_word = 'horrible'  \n",
    "print(f\"\\nConcordances for word '{specific_word}':\")\n",
    "text_obj.concordance(specific_word, lines=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Common Trigrams in Frequency Distribution:\n",
      "im gon na: 6318\n",
      "wan na go: 3993\n",
      "dont wan na: 2948\n",
      "cant wait see: 2230\n",
      "happy mothers day: 2150\n",
      "gon na miss: 1820\n",
      "gon na go: 1781\n",
      "mtv movie awards: 1349\n",
      "wan na see: 1286\n",
      "dont want go: 1180\n",
      "got ta go: 1147\n",
      "gon na get: 1115\n",
      "got ta get: 986\n",
      "hope feel better: 945\n",
      "wish could go: 934\n",
      "getting ready go: 905\n",
      "cant wait till: 864\n",
      "im going miss: 794\n",
      "got ta love: 765\n",
      "dont feel good: 759\n",
      "\n",
      "Tabulation of Most Common Trigrams:\n",
      "        ('im', 'gon', 'na')         ('wan', 'na', 'go')       ('dont', 'wan', 'na')     ('cant', 'wait', 'see') ('happy', 'mothers', 'day') \n",
      "                       6318                        3993                        2948                        2230                        2150 \n"
     ]
    }
   ],
   "source": [
    "# A TrigramCollocationFinder instance was created from the filtered words\n",
    "trigram_finder = TrigramCollocationFinder.from_words(filtered_words)\n",
    "\n",
    "# Optionally apply filters to the finder ( filter out trigrams that occur only once)\n",
    "trigram_finder.apply_freq_filter(2) # trigrams that apear twice or more\n",
    "\n",
    "# Find and print the most common trigrams\n",
    "most_common_trigrams = trigram_finder.ngram_fd.most_common(20)\n",
    "print(\"\\nMost Common Trigrams in Frequency Distribution:\")\n",
    "for trigram, frequency in most_common_trigrams:\n",
    "    print(f\"{' '.join(trigram)}: {frequency}\")\n",
    "\n",
    "# Tabulate the most common trigrams \n",
    "print(\"\\nTabulation of Most Common Trigrams:\")\n",
    "trigram_finder.ngram_fd.tabulate(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\swast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>polarity_scores</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467812025</td>\n",
       "      <td>2009-04-07T06:20:09.000+01:00</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>que me muera</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1468032862</td>\n",
       "      <td>2009-04-07T07:21:35.000+01:00</td>\n",
       "      <td>lamarmcarter</td>\n",
       "      <td>happy for coach stringer hof co now if i can o...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.802, 'pos': 0.198, 'comp...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.5719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1468043104</td>\n",
       "      <td>2009-04-07T07:24:53.000+01:00</td>\n",
       "      <td>Geoffasaurus</td>\n",
       "      <td>keeeerrrrriiiiii i really have nothing better ...</td>\n",
       "      <td>{'neg': 0.151, 'neu': 0.849, 'pos': 0.0, 'comp...</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1468078493</td>\n",
       "      <td>2009-04-07T07:35:44.000+01:00</td>\n",
       "      <td>kennyduduxd</td>\n",
       "      <td>cant sleep dam naps lol and its hot</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.714, 'pos': 0.286, 'comp...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1468242583</td>\n",
       "      <td>2009-04-07T08:30:07.000+01:00</td>\n",
       "      <td>Tsukihysteria</td>\n",
       "      <td>thank you for being a great character on house...</td>\n",
       "      <td>{'neg': 0.15, 'neu': 0.531, 'pos': 0.319, 'com...</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.5423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userid                      timestamp       username  \\\n",
       "0  1467812025  2009-04-07T06:20:09.000+01:00        mimismo   \n",
       "1  1468032862  2009-04-07T07:21:35.000+01:00   lamarmcarter   \n",
       "2  1468043104  2009-04-07T07:24:53.000+01:00   Geoffasaurus   \n",
       "3  1468078493  2009-04-07T07:35:44.000+01:00    kennyduduxd   \n",
       "4  1468242583  2009-04-07T08:30:07.000+01:00  Tsukihysteria   \n",
       "\n",
       "                                               tweet  \\\n",
       "0                                       que me muera   \n",
       "1  happy for coach stringer hof co now if i can o...   \n",
       "2  keeeerrrrriiiiii i really have nothing better ...   \n",
       "3                cant sleep dam naps lol and its hot   \n",
       "4  thank you for being a great character on house...   \n",
       "\n",
       "                                     polarity_scores  negative  neutral  \\\n",
       "0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...     0.000    1.000   \n",
       "1  {'neg': 0.0, 'neu': 0.802, 'pos': 0.198, 'comp...     0.000    0.802   \n",
       "2  {'neg': 0.151, 'neu': 0.849, 'pos': 0.0, 'comp...     0.151    0.849   \n",
       "3  {'neg': 0.0, 'neu': 0.714, 'pos': 0.286, 'comp...     0.000    0.714   \n",
       "4  {'neg': 0.15, 'neu': 0.531, 'pos': 0.319, 'com...     0.150    0.531   \n",
       "\n",
       "   positive  compound  \n",
       "0     0.000    0.0000  \n",
       "1     0.198    0.5719  \n",
       "2     0.000   -0.3959  \n",
       "3     0.286    0.4215  \n",
       "4     0.319    0.5423  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# SentimentIntensityAnalyzer initialization \n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_polarity_scores(tweet):\n",
    "    # Convert to string if the tweet is not already a string\n",
    "    if not isinstance(tweet, str):\n",
    "        tweet = str(tweet)\n",
    "    return sia.polarity_scores(tweet)\n",
    "\n",
    "# Apply the function to the 'tweet' column to get the scores\n",
    "df['polarity_scores'] = df['tweet'].apply(get_polarity_scores)\n",
    "\n",
    "# Expand the dictionary into separate columns\n",
    "df['negative'] = df['polarity_scores'].apply(lambda score_dict: score_dict['neg'])\n",
    "df['neutral'] = df['polarity_scores'].apply(lambda score_dict: score_dict['neu'])\n",
    "df['positive'] = df['polarity_scores'].apply(lambda score_dict: score_dict['pos'])\n",
    "df['compound'] = df['polarity_scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Compound Score for the entire dataset: 0.130014422708883\n"
     ]
    }
   ],
   "source": [
    "# compound score into a separate column\n",
    "df['compound'] = df['polarity_scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "# Mean of the compound scores\n",
    "average_compound_score = df['compound'].mean()\n",
    "\n",
    "print(\"Average Compound Score for the entire dataset:\", average_compound_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTKâ€™s Pre-Trained Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def is_positive(tweet: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(tweet)[\"compound\"] > 0\n",
    "\n",
    "shuffle(tweets)\n",
    "for tweet in tweets[:10]:\n",
    "    print(\">\", is_positive(tweet), tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "all_review_ids = positive_review_ids + negative_review_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def is_positive(review_id: str) -> bool:\n",
    "    \"\"\"True if the average of all sentence compound scores is positive.\"\"\"\n",
    "    text = nltk.corpus.movie_reviews.raw(review_id)\n",
    "    scores = [\n",
    "        sia.polarity_scores(sentence)[\"compound\"]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    return mean(scores) > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(all_review_ids)\n",
    "correct = 0\n",
    "for review_id in all_review_ids:\n",
    "     if is_positive(review_id):\n",
    "         if review_id in positive_review_ids:\n",
    "             correct += 1\n",
    "     else:\n",
    "         if review_id in negative_review_ids:\n",
    "             correct += 1\n",
    "\n",
    "print(F\"{correct / len(all_review_ids):.2%} correct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    ")]\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    ")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_fd = nltk.FreqDist(positive_words)\n",
    "negative_fd = nltk.FreqDist(negative_words)\n",
    "\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "top_100_positive = {word for word, count in positive_fd.most_common(100)}\n",
    "top_100_negative = {word for word, count in negative_fd.most_common(100)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "positive_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"pos\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])\n",
    "negative_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"neg\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    positive_scores = list()\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in top_100_positive:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    # since some classifiers you'll use later don't work with negative numbers.\n",
    "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use 1/4 of the set for training\n",
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "classifier = nltk.NaiveBayesClassifier.train(features[:train_count])\n",
    "classifier.show_most_informative_features(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    ComplementNB,\n",
    "    MultinomialNB,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 1/4 of the set for training\n",
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "for name, sklearn_classifier in classifiers.items():\n",
    "     classifier = nltk.classify.SklearnClassifier(sklearn_classifier)\n",
    "     classifier.train(features[:train_count])\n",
    "     accuracy = nltk.classify.accuracy(classifier, features[train_count:])\n",
    "     print(F\"{accuracy:.2%} - {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
